{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Semantic Analysis tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **SemanticDetector** class is used to predict a sentiment score in a document / email.\n",
    "\n",
    "For that purpose, two inputs are required:\n",
    "- a list of seed words that caracterize a sentiment \n",
    "  Exemple : the seed words [\"mad\", \"furious\", \"insane\"] caracterize the sentiment \"dissatisfaction\"\n",
    "- a trained embedding (Melusine **Embedding** class instance) to compute distances between words/tokens\n",
    "\n",
    "The three steps for sentiment score prediction are the following:\n",
    "- Instanciate a SentimentDetector object with a list of seed words as argument\n",
    "- Use the SentimentDetector.fit method (with an embedding object as argument) to compute the lexicons\n",
    "- Use the SentimentDetector.predict method on a document/email DataFrame to predict the sentiment score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal working exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:33.699468Z",
     "start_time": "2019-12-16T14:35:30.690799Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP tools\n",
    "from melusine.nlp_tools.embedding import Embedding\n",
    "from melusine.nlp_tools.tokenizer import Tokenizer\n",
    "\n",
    "# Models\n",
    "from melusine.models.modeler_semantic import SemanticDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load email data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:33.717671Z",
     "start_time": "2019-12-16T14:35:33.703088Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails_clean = pd.read_csv('./data/emails_preprocessed.csv', encoding='utf-8', sep=';')\n",
    "df_emails_clean = df_emails_clean[['clean_body']]\n",
    "df_emails_clean = df_emails_clean.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:33.904225Z",
     "start_time": "2019-12-16T14:35:33.720025Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/05 11:28 - melusine.nlp_tools.embedding - INFO - Start training for embedding\n",
      "21/05 11:28 - melusine.nlp_tools.embedding - INFO - Done.\n"
     ]
    }
   ],
   "source": [
    "# Train an embedding using the text data in the 'clean_body' column\n",
    "embedding = Embedding(input_column='clean_body', vector_size=300, min_count=2)\n",
    "embedding.train(df_emails_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:33.923300Z",
     "start_time": "2019-12-16T14:35:33.906376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'vehicule', 'flag_date_']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a list of words present in the Embedding vocabulary\n",
    "list(embedding.embedding.wv.key_to_index.keys())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:33.933104Z",
     "start_time": "2019-12-16T14:35:33.925475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pouvoir', 0.14628919959068298),\n",
       " ('merci', 0.11492132395505905),\n",
       " ('studio', 0.1143675297498703)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the trained embedding : print most similar words\n",
    "embedding.embedding.wv.most_similar('client', topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:33.959147Z",
     "start_time": "2019-12-16T14:35:33.936240Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the text in the clean_body column\n",
    "tokenizer = Tokenizer (input_column='clean_body', stop_removal=True, n_jobs=20)\n",
    "df_emails_clean = tokenizer.fit_transform(df_emails_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:33.970050Z",
     "start_time": "2019-12-16T14:35:33.961038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [client, chez, pouvez, etablir, devis, fils, s...\n",
       "1    [informe, nouvelle, immatriculation, enfin, fa...\n",
       "2    [suite, a, conversation, telephonique, flag_da...\n",
       "3    [fais, suite, a, mail, envoye, bulletin, salai...\n",
       "4    [voici, ci, joint, bulletin, salaire, comme, d...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tokenizer : print tokens\n",
    "df_emails_clean['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate and fit the Sentiment Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:33.981006Z",
     "start_time": "2019-12-16T14:35:33.972684Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_word_list = ['immatriculation']\n",
    "\n",
    "# Instanciate a SentimentDetector object\n",
    "semantic_detector = SemanticDetector(base_seed_words=seed_word_list, tokens_column='tokens')\n",
    "\n",
    "# Fit the SentimentDetector using the trained embedding\n",
    "semantic_detector.fit(embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:33.995038Z",
     "start_time": "2019-12-16T14:35:33.987348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of seed words:\n",
      "['immatriculation']\n"
     ]
    }
   ],
   "source": [
    "print('List of seed words:')\n",
    "print(semantic_detector.seed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:34.018525Z",
     "start_time": "2019-12-16T14:35:34.007873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Part of) Lexicon associated with the seed words \"immatriculation\":\n",
      "  00 : -0.1417384296655655\n",
      "  1 : -0.010289491154253483\n",
      "  2 : -0.019492965191602707\n",
      "  a : 0.0005874728667549789\n",
      "  adresse : 0.0450427271425724\n",
      "  afin : 0.08064097911119461\n",
      "  ainsi : 0.08764959126710892\n",
      "  assurance : -0.03274295851588249\n",
      "  assurer : 0.02226885035634041\n",
      "  attached : -0.05434000864624977\n"
     ]
    }
   ],
   "source": [
    "seed_word = semantic_detector.seed_list[0]\n",
    "lexicon = semantic_detector.lexicon\n",
    "sorted_lexicon = dict(sorted(lexicon.items(), key = lambda x: x[0]))\n",
    "\n",
    "print(f'(Part of) Lexicon associated with the seed words \"{\", \".join(semantic_detector.seed_list)}\":')\n",
    "for word, sentiment_score in list(sorted_lexicon.items())[:10]:\n",
    "    print('  ' + word + ' : ' + str(sentiment_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and print the sentiment score\n",
    "\n",
    "**Warning :** In this exemple, the embedding is trained on a corpus of 40 emails which is WAY too small to yield valuable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:34.049618Z",
     "start_time": "2019-12-16T14:35:34.022250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_body</th>\n",
       "      <th>tokens</th>\n",
       "      <th>semantic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>je vous informe que la nouvelle immatriculatio...</td>\n",
       "      <td>[informe, nouvelle, immatriculation, enfin, fa...</td>\n",
       "      <td>0.072973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suite a notre conversation telephonique de  fl...</td>\n",
       "      <td>[suite, a, conversation, telephonique, flag_da...</td>\n",
       "      <td>0.065651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voici ci joint mon bulletin de salaire comme d...</td>\n",
       "      <td>[voici, ci, joint, bulletin, salaire, comme, d...</td>\n",
       "      <td>0.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ci-joint le rib du compte comme demande</td>\n",
       "      <td>[ci-joint, rib, compte, comme, demande]</td>\n",
       "      <td>0.045614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ci-joint pret vehicule</td>\n",
       "      <td>[ci-joint, pret, vehicule]</td>\n",
       "      <td>0.044213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           clean_body  \\\n",
       "1   je vous informe que la nouvelle immatriculatio...   \n",
       "2   suite a notre conversation telephonique de  fl...   \n",
       "4   voici ci joint mon bulletin de salaire comme d...   \n",
       "26            ci-joint le rib du compte comme demande   \n",
       "9                              ci-joint pret vehicule   \n",
       "\n",
       "                                               tokens  semantic_score  \n",
       "1   [informe, nouvelle, immatriculation, enfin, fa...        0.072973  \n",
       "2   [suite, a, conversation, telephonique, flag_da...        0.065651  \n",
       "4   [voici, ci, joint, bulletin, salaire, comme, d...        0.055700  \n",
       "26            [ci-joint, rib, compte, comme, demande]        0.045614  \n",
       "9                          [ci-joint, pret, vehicule]        0.044213  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the name of the column returned (default is \"score\")\n",
    "return_column = \"semantic_score\"\n",
    "\n",
    "# Predict the sentiment score on each email of the DataFrame\n",
    "df_emails_clean = semantic_detector.predict(df_emails_clean, return_column=return_column)\n",
    "\n",
    "# Print emails with the maximum sentiment score\n",
    "df_emails_clean.sort_values(by=return_column, ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SentimentDetector class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SemanticDetector class provides an unsupervised methodology to assign a sentiment score to a corpus of documents/emails. The methodology used to predict a sentiment score using the SemanticDetector is described below:\n",
    "\n",
    "1. **Define a list of seed words that caracterize a sentiment**\n",
    "    - Take a list of seed words as input\n",
    "    - If the `extend_seed_word_list` parameter is set to True: extend the list of seed words with words sharing the same root (dance -> [\"dancing\", \"dancer\"])  \n",
    "    \n",
    "    \n",
    "2. **Fit the model (= create a lexicon to assign a score for every word in the vocabulary)**\n",
    "    - Create a lexicon for each seed word by computing the cosine similarity between the seed word and all the words in the vocabulary is computed.\n",
    "    - Aggregate the similarity score obtained for the different seed words in a unique lexicon\n",
    "    - (To compute cosine similarities, a trained embedding is required.)  \n",
    "    \n",
    "3. **Predict a sentiment score for emails/documents**\n",
    "    - Filter out the tokens in the document that are not in the vocabulary.\n",
    "    - For each remaining token, compute its sentiment score using the lexicon.\n",
    "    - For each email, aggregate the score accross different tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of a SemanticDetector object are :\n",
    "    \n",
    "- **base_seed_words :** the list of seed words that caracterize a sentiment/theme\n",
    "- **base_anti_seed_words :** the list of seed words that caracterize undesired sentiments/themes\n",
    "- **anti_weight :** the weight of anti_seeds in the computation of the semantic score\n",
    "- **tokens_column :** name of the column in the input DataFrame that contains tokens\n",
    "- **extend_seed_word_list :** if True: complement seed words with words sharing the same root (dance -> [\"dancing\", \"dancer\"]). Default value False.\n",
    "- **normalize_scores :** if True: normalize the lexicon scores of eache word. Default value False.\n",
    "- **aggregation_function_seed_wise :** Function to aggregate the scores associated with a token accross the different seeds. Default function is a max.\n",
    "- **aggregation_function_email_wise :** Function to aggregate the scores associated with the different tokens in an email. Default function is the 60th percentile.\n",
    "- **n_jobs :** the number of cores used for computation. Default value, 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out undesired themes with using \"anti seed words\" and \"anti_ratio\"\n",
    "\n",
    "If you want to detect emergency in your emails, you could use the seed word `\"emergency\"`.  \n",
    "* \"I need an answer, this is an emergency !!\" => Semantic score = 0.98   \n",
    "\n",
    "But you might detect undesired sentences such as:\n",
    "* \"Yesterday I tested the emergency brake of my car\" => Semantic score = 0.95  \n",
    "\n",
    "You can prevent the detection of undesired themes using anti seed words:  \n",
    "* `base_anti_seed_word_list = ['brake']`\n",
    "* \"Yesterday I tested the emergency brake of my car\" => Semantic score = 0.50  \n",
    "\n",
    "You can control the contribution of anti seed words using the `anti_weight` (default 0.3):  \n",
    "* `base_anti_seed_word_list = ['brake']`\n",
    "* `anti_weight = 0.6`\n",
    "* \"Yesterday I tested the emergency brake of my car\" => Semantic score = 0.30  \n",
    "\n",
    "The formula used to compute the semantic score is:  \n",
    "* semantic score = seed_word_contrib - anti_weight * anti_seed_word_contrib  \n",
    "\n",
    "Warning : an `anti_weight` above one means anti seeds contribute more (negatively) than regular seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:34.068729Z",
     "start_time": "2019-12-16T14:35:34.052708Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_word_list = ['immatriculation']\n",
    "anti_seed_word_list = ['demandes']\n",
    "\n",
    "\n",
    "# Instanciate SentimentDetector objects\n",
    "regular_semantic_detector = SemanticDetector(base_seed_words=seed_word_list, tokens_column='tokens')\n",
    "semantic_detector_with_anti = SemanticDetector(base_seed_words=seed_word_list, tokens_column='tokens', \n",
    "                                               base_anti_seed_words = anti_seed_word_list)\n",
    "semantic_detector_with_anti2 = SemanticDetector(base_seed_words=seed_word_list, tokens_column='tokens', \n",
    "                                               base_anti_seed_words = anti_seed_word_list, anti_weight=0.5)\n",
    "\n",
    "\n",
    "# Fit the SentimentDetectors using the trained embedding\n",
    "regular_semantic_detector.fit(embedding=embedding)\n",
    "semantic_detector_with_anti.fit(embedding=embedding)\n",
    "semantic_detector_with_anti2.fit(embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:34.110119Z",
     "start_time": "2019-12-16T14:35:34.070276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_body</th>\n",
       "      <th>tokens</th>\n",
       "      <th>semantic_score</th>\n",
       "      <th>semantic_score with anti (anti_weight=0.3)</th>\n",
       "      <th>semantic_score with anti (anti_weight=0.5)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>je vous informe que la nouvelle immatriculatio...</td>\n",
       "      <td>[informe, nouvelle, immatriculation, enfin, fa...</td>\n",
       "      <td>0.072973</td>\n",
       "      <td>0.052712</td>\n",
       "      <td>0.036220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suite a notre conversation telephonique de  fl...</td>\n",
       "      <td>[suite, a, conversation, telephonique, flag_da...</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>0.076134</td>\n",
       "      <td>0.075565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voici ci joint mon bulletin de salaire comme d...</td>\n",
       "      <td>[voici, ci, joint, bulletin, salaire, comme, d...</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.056599</td>\n",
       "      <td>0.057199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ci-joint le rib du compte comme demande</td>\n",
       "      <td>[ci-joint, rib, compte, comme, demande]</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>0.044363</td>\n",
       "      <td>0.043529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ci-joint pret vehicule</td>\n",
       "      <td>[ci-joint, pret, vehicule]</td>\n",
       "      <td>0.044213</td>\n",
       "      <td>0.041173</td>\n",
       "      <td>0.039147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           clean_body  \\\n",
       "1   je vous informe que la nouvelle immatriculatio...   \n",
       "2   suite a notre conversation telephonique de  fl...   \n",
       "4   voici ci joint mon bulletin de salaire comme d...   \n",
       "26            ci-joint le rib du compte comme demande   \n",
       "9                              ci-joint pret vehicule   \n",
       "\n",
       "                                               tokens  semantic_score  \\\n",
       "1   [informe, nouvelle, immatriculation, enfin, fa...        0.072973   \n",
       "2   [suite, a, conversation, telephonique, flag_da...        0.065651   \n",
       "4   [voici, ci, joint, bulletin, salaire, comme, d...        0.055700   \n",
       "26            [ci-joint, rib, compte, comme, demande]        0.045614   \n",
       "9                          [ci-joint, pret, vehicule]        0.044213   \n",
       "\n",
       "    semantic_score with anti (anti_weight=0.3)  \\\n",
       "1                                     0.052712   \n",
       "2                                     0.076134   \n",
       "4                                     0.056599   \n",
       "26                                    0.044363   \n",
       "9                                     0.041173   \n",
       "\n",
       "    semantic_score with anti (anti_weight=0.5)  \n",
       "1                                     0.036220  \n",
       "2                                     0.075565  \n",
       "4                                     0.057199  \n",
       "26                                    0.043529  \n",
       "9                                     0.039147  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the name of the column returned (default is \"score\")\n",
    "return_column1 = \"semantic_score\"\n",
    "return_column2 = \"semantic_score with anti (anti_weight=0.3)\"\n",
    "return_column3 = \"semantic_score with anti (anti_weight=0.5)\"\n",
    "\n",
    "\n",
    "\n",
    "# Predict the sentiment score on each email of the DataFrame\n",
    "df_emails_clean = regular_semantic_detector.predict(df_emails_clean, return_column=return_column1)\n",
    "df_emails_clean = semantic_detector_with_anti.predict(df_emails_clean, return_column=return_column2)\n",
    "df_emails_clean = semantic_detector_with_anti2.predict(df_emails_clean, return_column=return_column3)\n",
    "\n",
    "\n",
    "# Print emails with the maximum sentiment score\n",
    "df_emails_clean.sort_values(by=return_column1, ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find extra seed words with the `extend_seed_word_list` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SentimentDetector \"extend_seed_word_list\" parameter activates the search for extra seed words sharing the same root as the base seed words.  \n",
    "\n",
    "For example, if \"dance\" is a base seed word, \"extend_seed_word_list\" will loop through the words in the embedding vocabulary and find new seed words such as \"dancer\", \"dancing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:34.127464Z",
     "start_time": "2019-12-16T14:35:34.112397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate a SentimentDetector object\n",
    "semantic_detector_extended_seed = SemanticDetector(\n",
    "    base_seed_words=['tel', 'assur'], tokens_column='tokens', extend_seed_word_list=True)\n",
    "\n",
    "# Fit the SentimentDetector using the trained embedding\n",
    "semantic_detector_extended_seed.fit(embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:34.133308Z",
     "start_time": "2019-12-16T14:35:34.129157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tel': ['telephonique', 'telephone', 'tel'], 'assur': ['assurance', 'assurer']}\n",
      "['telephonique', 'telephone', 'tel', 'assurance', 'assurer']\n"
     ]
    }
   ],
   "source": [
    "# Print the extended list of seed words\n",
    "print(semantic_detector_extended_seed.seed_dict)\n",
    "print(semantic_detector_extended_seed.seed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a custom function to aggregate lexicon scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate token score over seeds\n",
    "The SemanticDetector computes a similarity between a word and every seed words.  \n",
    "An aggretion function is then used to keep a single score for each token.  \n",
    "\n",
    "Exemple : \n",
    "- Seed word list : [\"horse\", \"animal\"]\n",
    "- Embedding : simulated\n",
    "\n",
    "Lexicon \"horse\" :\n",
    "{\n",
    "  \"apple\" : 0.2,\n",
    "  ...\n",
    "  \"hello\" : 0.1,\n",
    "  ...\n",
    "  \"ponies\" : 8.8,\n",
    "  ...\n",
    "  \"zebra\" : 1.2\n",
    "}  \n",
    "Lexicon \"animal\" :\n",
    "{\n",
    "  \"apple\" : 0.1,\n",
    "  ...\n",
    "  \"hello\" : 0.3,\n",
    "  ...\n",
    "  \"ponies\" : 4.8,\n",
    "  ...\n",
    "  \"zebra\" : 6.2\n",
    "}\n",
    "\n",
    "**Aggregated Lexicon :**  \n",
    "{\n",
    "  \"apple\" : 0.2,\n",
    "  ...\n",
    "  \"hello\" : 0.3,\n",
    "  ...\n",
    "  \"ponies\" : 8.8,\n",
    "  ...\n",
    "  \"zebra\" : 6.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate semantic score over tokens\n",
    "When evaluating an email, each word in the email has an associated score.  \n",
    "An aggregation function is used to keep a single score for each email.  \n",
    "\n",
    "Exemple : \n",
    "- Sentence : \"Hello, I like ponies\"\n",
    "- Seed word list : [\"horse\", \"animal\"]\n",
    "- Embedding : simulated\n",
    "\n",
    "**Sentence score :**  \n",
    "- score : score(Hello) + score(I) + score(like) + score(ponies)\n",
    "- score : 0.3 + 0.1 + 0.2 + 8.8 = 9.4\n",
    "\n",
    "\n",
    "The semantic score for the email is thus 9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default aggregation functions\n",
    "\n",
    "The default aggregation methodology is the following:  \n",
    "- Seed-wise aggregation : For a token, take the max score accross seed\n",
    "  - Exemple :  \n",
    "    ponies_score = 8.8 (lexicon \"horse\")   \n",
    "    ponies_score = 4.8 (lexicon \"animal\")  \n",
    "    => Score for the \"ponies\" token = np.max(8.8, 4.8) = 8.8  \n",
    "  \n",
    "  \n",
    "- Email-wise aggregation : Given a list of token scores, take the percentile 60 as the sentiment_score for the email\n",
    "  - Exemple :  \n",
    "    token_score_list : [0.3 (hello), 0.3 (i), 0.2 (ponies)]  \n",
    "    => sentiment_score = np.percentile([0.3, 0.3, 0.2, 8.8], 60) = 0.3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:34.151385Z",
     "start_time": "2019-12-16T14:35:34.135396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate a SentimentDetector object with custom aggregation function:\n",
    "# - A mean for the seed-wise aggregation\n",
    "# - A 95th percentile for the email-wise aggregation\n",
    "\n",
    "def aggregation_mean(x):\n",
    "    return np.mean(x, axis=0)\n",
    "\n",
    "def aggregation_percentile_95(x):\n",
    "    return np.percentile(x, 95)\n",
    "\n",
    "semantic_detector_custom_aggregation = SemanticDetector(\n",
    "    base_seed_words=['client'], \n",
    "    tokens_column='tokens', \n",
    "    aggregation_function_seed_wise=aggregation_mean,\n",
    "    aggregation_function_email_wise=aggregation_percentile_95\n",
    ")\n",
    "\n",
    "# Fit the SentimentDetector using the trained embedding\n",
    "semantic_detector_custom_aggregation.fit(embedding=embedding)\n",
    "\n",
    "# Predict the sentiment score on each email of the DataFrame\n",
    "df_emails_clean_custom_aggregation = semantic_detector_custom_aggregation.predict(df_emails_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T14:35:35.633842Z",
     "start_time": "2019-12-16T14:35:34.153105Z"
    }
   },
   "outputs": [],
   "source": [
    "semantic_detector_multiprocessing = SemanticDetector(\n",
    "    base_seed_words=['certificat'], \n",
    "    tokens_column='tokens', \n",
    "    n_jobs = 2\n",
    ")\n",
    "\n",
    "# Fit the SentimentDetector using the trained embedding\n",
    "semantic_detector_multiprocessing.fit(embedding=embedding)\n",
    "\n",
    "# Predict the sentiment score on each email of the DataFrame\n",
    "df_emails_multiprocessing = semantic_detector_multiprocessing.predict(df_emails_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing2",
   "language": "python",
   "name": "testing2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
